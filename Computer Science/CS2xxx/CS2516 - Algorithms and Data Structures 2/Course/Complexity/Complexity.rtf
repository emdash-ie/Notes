{\rtf1\ansi\deff0{\fonttbl{\f0 \fswiss Helvetica;}{\f1 Courier;}}
{\colortbl;\red255\green0\blue0;\red0\green0\blue255;}
\widowctrl\hyphauto

{\pard \ql \f0 \sa180 \li0 \fi0 \b \fs36 Complexity Analysis\par}
{\pard \ql \f0 \sa180 \li0 \fi0 Programs that take too long to finish are useless \u8211- good programming skills will not compensate for poor algorithms.\par}
{\pard \ql \f0 \sa180 \li0 \fi0 We need to understand how much work our algorithms will require for different inputs \u8211- learn to avoid bad or inefficient design patterns.\par}
{\pard \ql \f0 \sa180 \li0 \fi0 We also need to understand the limits \u8211- some problems can't be solved without at least a minimum amount of work. Don't waste time trying to design something that is impossible to achieve.\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \b \fs32 Revision\par}
{\pard \ql \f0 \sa180 \li0 \fi0 We measure complexity in terms of the number of basic steps \u8211-\u160?quick constant-time operations:\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab reading a value\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab assigning a value\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab comparing two values\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab simple arithmetic operations\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab calling a function\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab returning a value\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 Our main concern is wort-case complexity:\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab We want to know how bad it could get, and use that as a performance guarantee\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab The complexity is based on the size of the input, n \u8211-\u160?how many items in a list, or in a tree, or rows in a file\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab We are only concern about large inputs\par}
{\pard \ql \f0 \sa0 \li720 \fi-360 \endash \tx360\tab asymptotic analysis: as n tends to infinity, how many steps as a function of n?\sa180\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \b \fs32 Big O\par}
{\pard \ql \f0 \sa180 \li0 \fi0 For two functions f and g operating on positive integers, f(n) is O(g(n)) is there is an integer constant k >= 0 and a real constant C > 0 such that for all n bigger than k:\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \f1 f(n) <= C*g(n)\par}
{\pard \ql \f0 \sa180 \li0 \fi0 We can think of O(g(n)) as specifying a set of functions that are not significantly worse than g.\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \b \fs28 Standard Function Hierarchy\par}
{\pard \ql \f0 \sa180 \li0 \fi0 O(c) C O(log(n)) C O(n) C O(nlogn) C O(n^2) C O(n^3) ... C O(2^n) C O(n!)\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab Polynomials are classified by their highest degree.\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 It is correct to say that n^2 + 3n + 5 is O(n^3), but that's not as useful as saying that it's O(n^2).\par}
{\pard \ql \f0 \sa180 \li0 \fi0 Though sometimes looser bounds are all you need and are easier to guarantee from looking at your code.\par}
{\pard \ql \f0 \sa180 \li0 \fi0 There is a massive difference between a polynomial bound and O(2^n), which is taken as the boundary between efficient and non-efficient algorithms.\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \b \fs32 Big Omega\par}
{\pard \ql \f0 \sa180 \li0 \fi0 Never better than a constant multiple.\par}
{\pard \ql \f0 \sa180 \li0 \fi0 f(n) is \u937?(g(n)) if there is an integer constant k >= 0 and a real constant C > 0 so that for all n bigger than k [\u8230?]\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \b \fs28 Example: Prove fibonacci(n) is \u937?((3/2)^n)\par}
{\pard \ql \f0 \sa180 \li0 \fi0 For all n >= 3, fib(n) > fib(n-1).\par}
{\pard \ql \f0 \sa180 \li0 \fi0 We know fib(n) = fib(n-1) + fib(n-2).\par}
{\pard \ql \f0 \sa180 \li0 \fi0 So for all n >=4, fib(n) < fib(n-1) + fib(n-1)\par}
{\pard \ql \f0 \sa180 \li0 \fi0 So fib(n) < 2*fib(n-1)\par}
{\pard \ql \f0 \sa180 \li0 \fi0 So 1/2 * fib(n) < fib(n-1)\par}
{\pard \ql \f0 \sa180 \li0 \fi0 Now take k = 2 and C = 1/2.\par}
{\pard \ql \f0 \sa180 \li0 \fi0 When n = 3, fib(n) = 2 and C * (2/3)^n = 27/16 which is < 2\par}
{\pard \ql \f0 \sa180 \li0 \fi0 Assume true for n = p for some p >= 3. Consider n = p + 1.\par}
{\pard \ql \f0 \sa180 \li0 \fi0 fib(p + 1) = fib(p) + fib(p-1)\par}
{\pard \ql \f0 \sa180 \li0 \fi0 fib(p+1) > fib(p) + 1/2 * fib(p) which is = 3/2 * fib(p)\par}
{\pard \ql \f0 \sa180 \li0 \fi0 fib(p+1) > 1/2 * (3/2 ^ p+1)\par}
{\pard \ql \f0 \sa180 \li0 \fi0 So result true by induction.\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \b \fs32 Big Theta\par}
{\pard \ql \f0 \sa180 \li0 \fi0 Has an upper and lower bound, showing that f is not significantly different from g.\par}
{\pard \ql \f0 \sa180 \li0 \fi0 If you can prove that something is O(f(n)) and \u937?(f(n)), then you already have Theta(f(n)).\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \b \fs28 Example: n^2 + 3n + 5\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab We already know this is O(n^2).\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 Now pick k = 0.\par}
{\pard \ql \f0 \sa180 \li0 \fi0 n^2 + 3n + 5 <= n^2 + 3n^2 + 5n^2 = 9n^2\par}
{\pard \ql \f0 \sa180 \li0 \fi0 C2 = 9\par}
{\pard \ql \f0 \sa180 \li0 \fi0 n^2 <= n^2 + 3n + 5\par}
{\pard \ql \f0 \sa180 \li0 \fi0 So C1 = 1\par}
{\pard \ql \f0 \sa180 \li0 \fi0 It's always between n^2 and 9n^2.\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \b \fs32 Amortised Analysis\par}
{\pard \ql \f0 \sa180 \li0 \fi0 The true cost to the CPU of appending to a Python list of size k:\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab if there is space, c units of time to assign value to the next cell\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab If there is not space, then:\par}
{\pard \ql \f0 \sa0 \li720 \fi-360 \endash \tx360\tab kc units of time to copy k values across to new list\par}
{\pard \ql \f0 \sa0 \li720 \fi-360 \endash \tx360\tab c units of time to assign the value\sa180\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 In the analysis, we will charge a fixed cost for each append no matter what space is available or what size the list is.\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab For simple appends, this builds up a profit\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab Which we then spend on the complex appends\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 How much should we charge to ensure that we never run at a loss (for big enough lists)?\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab This value is the amortised complexity of the operation.\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 Assume the fixed c units of time are worth \u8364?1. Simple append is \u8364?1, complex append is \u8364?(k+1). Let's charge \u8364?3 per append.\par}
{\pard \ql \f0 \sa180 \li0 \fi0 [get this bit]\par}
{\pard \ql \f0 \sa180 \li0 \fi0 In general, you want the charge to be as low as possible to still cover the cost.\par}
{\pard \ql \f0 \sa180 \li0 \fi0 For amortised complexity, you use an {\f1 *}: {\f1 O(1)*}\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \b \fs32 Recursive Function Complexity\par}
{\pard \ql \f0 \sa180 \li0 \fi0 Basic approach:\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 1.\tx360\tab count the worst-case work done by a single activation, without the recursive call\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 2.\tx360\tab count the worst-case number of recursive calls\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 3.\tx360\tab multiply the two counts together\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \b \fs28 Example: Powers\par}
{\pard \ql \f0 \sa180 \li0 \fi0 Instead of stepping through powers one at a time: x^n * x^n-1 * \u8230?\par}
{\pard \ql \f0 \sa180 \li0 \fi0 We do this: x^n/2 * x^n/2\par}
{\pard \ql \f0 \sa180 \li0 \fi0 This means instead of n recursive calls, we have log(n), with still a constant number of operations in each call.\par}
{\pard \ql \f0 \sa180 \li0 \fi0 This is significantly faster, and reduces the number of calls on the activation stack, meaning you're less likely to hit the memory limit.\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \b \fs28 Example: du\par}
{\pard \ql \f0 \sa180 \li0 \fi0 {\f1 du} is a Linux utility to calculate the size of a directory subtree.\par}
{\pard \ql \f0 \sa180 \li0 \fi0 If there are n total directories, then each directory will be processed once and only once, and if there are t files in total, then the algorithm will be O(n+t).\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \b \fs32 More on Recursion Complexity\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \b \fs28 Example: Fibonacci\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \f1     def fib1(n):\line
        if n < 2:\line
            return 1\line
        elif n == 2:\line
            return 1\line
        return fib1(n-1) + fib1(n-2)\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \b \fs24 Analysis\par}
{\pard \ql \f0 \sa180 \li0 \fi0 All constant except the recursive calls, and we need two recursive calls per activation.\par}
{\pard \ql \f0 \sa180 \li0 \fi0 However, the 2nd is repeated inside the first. We're calling fib(n-2) again inside fib1(n-1), meaning we're calling it twice.\par}
{\pard \ql \f0 \sa180 \li0 \fi0 E.g. to get fib(7) we have to get fib(6) and fib(5). [etc]\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab fib(7) and fib(6) are called once\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab fib(5) is called twice\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab fib(4) is called 3 times\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab fib(3) is called 5 times\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab fib(2) is called 8 times\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 This is extremely inefficient. It can't do numbers higher than about 12.\par}
{\pard \ql \f0 \sa180 \li0 \fi0 The number of calls for f(n) is > 2 ^(n/2).\par}
{\pard \ql \f0 \sa180 \li0 \fi0 So the algorithm is \u937?(2^n) [check]\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \b \fs28 Example: Efficient Fibonacci\par}
{\pard \ql \f0 \sa180 \li0 \fi0 Each call to fib(n) computes fib(n-1) also. We will return them both.\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \f1     def fib(n):\line
        (a,b) = _fib(n)\line
        return a\line
\line
    def _fib(n):\line
        if n == 1:\line
            return (1, 0)\line
        elif n == 2:\line
            return (1, 1)\line
        (a, b) = _fib(n-1)\line
        return (a + b, a)\par}
{\pard \ql \f0 \sa180 \li0 \fi0 This is linear.\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \b \fs28 Exercise: More Efficient Fibonacci Using Matrices\par}
{\pard \ql \f0 \sa180 \li0 \fi0 [check]\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab You can get it to O(log(n)) using matrices and a trick from lecture 2.\sa180\par}
}
