# Exploiting Memory Hierarchy: Main Memory, Associative Cache

## Write Allocation

[missed the start of this]

A write miss is when the processor wants to write data into memory that isn't there(?).

The memory needs to be fetched into a buffer and then overwritten.

An alternative is write-around – don't fetch the block, just write into the main memory. (no writing in the cache?)

For write-back:

* usually fetch the block

* use write buffer to avoid overwrite

    * write to the buffer, check if consistent, then write to main memory if it is

## Example: Intrinsity FastMATH

* embedded MIPS processor

    * 12-stage pipeline

    * uses one part of cache for instruction fetch and one part for data, which can operate in parallel

        * called a split cache strategy

The miss rate is much lower for the instruction cache (because it's mostly reading?).

## Main Memory Supporting Caches

Use DRAMs for main memory:

* fixed width (e.g. 1 word)

* connected by fixed-width clocked bus

    * bus clock is typically slower than CPU clock

[…]

## Performance Summary

When CPU performance is increased, the miss penalty becomes more significant.

When the clock rate is increased, memory stalls account for more CPU cycles (because the CPU cannot progress unless the data is brought in).

Can't neglect cache behaviour when evaluating system performance.

## Associative Caches

Fully associative – allow a given block to go in any cache entry. (place in next available spot)

This requires all entries to be searched at once. (the cache has to be searched for the memory address passed down from the CPU)

Also requires a comparator per entry, which is expensive.

* placing data is simple but comparing it is expensive

## N-way Set Associative

* middle ground between fully associative and direct map

Each set contains n entries. Block number determines which set. (block number modulo `n`)

Search all entries in a given set at once.

Requires n comparators (less expensive than fully associative).

* can view direct map as 1-way set associative

## Associativity Example

* With the direct map, everything's a miss because 0 and 8 keep replacing each other.

* With 2-way set associative, the 6 replaces the 8 because the 8 is least recently used.

* With fully associative, we get the maximum number of hits.

    * however, it's slower to access because we have to search the whole thing

    * also, we need the extra hardware

So we want to increase associativity to decrease the miss rate, but only up to a point. The improvement from increasing associativity decreases as you go.

## Replacement Policy

For direct map, there's no choice.

For set associative, prefer a non-valid entry. If there isn't one, choose among entries in the set.

Can also use least-recently used, which is simple for 2-way, manageable for 4-way, and too hard after that.

For more sets, random replacement works well.

## Multilevel Caches

Primary cache (L1) is attached to the CPU and is fast but small.

L2 cache services misses from this – it's bigger but slower.

Main memory services misses from L2.

Some high end systems have an extra L3 as well.

Adding more caches means searching is more complicated – have to perform the search at every level (in the worst case).

Goals:

* reduce cache miss penalty

* close gap between fast CPU clock rate and long access time for DRAM

### L1 vs L2

Primary cache:

* focus on minimal hit time

L2 cache:

* focus on low miss rate to avoid main memory access
* hit time has less overall impact

### Interactions with Advanced CPUs

Out-of-order CPUs can execute instructions during cache miss:

* pending store stays in load/store unit

* dependent instructions wait in reservation stations

    * independent instructions continue

Now the effect of a miss depends on program data flow. This is much harder to analyse, so we use system simulation.

## Memory System Dependability

* How much you can depend on a memory system

Three things we can do with faults:

* avoid them

* tolerate them

* predict them (fault forecasting)

Faults can be intermittent or permanent.

### Dependability Measures

You can measure reliability using the mean time to failure (MTTF).

You can measure the service interruption using the mean time to repair (MTTR).

Combining these gives the mean time between failures:

* MTBF = MTTF + MTTR

We can define availability based on this:

* Availability = MTTF / (MTTF + MTTR)

We can improve the availability by:

* increasing MTTF

    * fault avoidance, fault tolerance, fault forecasting

* reducing MTTR

    * improved tools and processes for diagnosis and repair

### The Hamming Single Error Correction (SEC) Code

This is a redundancy scheme for memory. It implements fault tolerance.

The Hamming distance is the number of bits that are different between two bit patterns.

[check]

#### Encoding SEC

* there's a full example in the book

To calculate Hamming Error Correction Code:

* number the bits from the left, starting at 1
* all bit positions that are a power of 2 are parity bits
* all other bit positions are used for data bits

Then each parity bit checks certain data bits:

* p1 checks all numbers whose binary representation has a 1 at the end

    * so 1, 3, 5, 7, …

* p2 checks all numbers whose binary representation has a 1 in the second-last place
    * so 2, 3, 6, 7, …

* p3 checks all numbers whose binary representation has a 1 in the third-last place

    * so 4-7, 12-16, …

* etc. (laid out like a truth table)

To fit a word of size 2^n, you need n+1 parity bits.

    * so 4 parity bits to represent 8 bits, meaning 12 bits total space

To determine the value of a parity bit, count the number of 1s in the data bits it covers. If it's odd, set the parity bit to 1, if it's even, set it to 0.

## Virtual Machine

* host computer emulates guest operating system and machine resources

    * improved isolation of multiple guests

    * avoids security and reliability problems

    * aids sharing of resources

Virtualisation has some performance impact, and so is only feasible with modern high-performance computers.

### Virtual Machine Monitor (aka Hypervisor)

This is the heart of the virtual machine technology.

It maps virtual resources to physical resources (memory, I/O devices, CPU).

Guest code runs on native machine in user mode.

* traps to VMM on privileged instructions and access to protected resources

Guest OS may be different from host OS.

[more here]

### Instruction Set Support

* user and system processor modes

Privileged instructions are only available in system mode – trap to system if executed in user mode.

All physical resources are only accessible using privileged instructions (e.g. page tables, interrupt controls, I/O registers).

Current ISAs are adapting to virtualisation (as it wasn't prominent when they were created).

## Virtual Memory

Use main memory as a cache for disk storage.

* managed jointly by CPU hardware and the OS

Programs share main memory.

* each gets a private virtual address space holding its frequently used code and data

* protected from other programs

The CPU and the OS translate virtual addresses to physical addresses.

* virtual memory blocks are called pages

* virtual memory translation miss is called a page fault

    * requires going to the hard disk to load the block into virtual memory

### Address Translation

* virtual memory space can have more addresses than the physical address space

    * discrepancy handled by translation

### Page Fault Penalty

Page fault penalty for going to the disk is very high.

Try to minimise the page fault rate:

* fully associative placement

    * no pages are in contention (until you run out of space?)

    * search time is high

* smart replacement algorithms used by the operating system to track placement

### Page Tables

* attempt to make up for high search time from fully associative placement

* stores placement information

    * array of page table entries, indexed by virtual page number

    * page table register in CPU points to page table in physical memory

* If page is present in memory

    * page table entry stores the physical page number

    * also other status bits (referenced, dirty, etc.)

* if a page is not present

    * page table entry can refer to location in swap space on disk

Swap space: partition on hard drives in linux – faster to access than the rest of the drive.

#### Translation Using a Page Table

Valid bit marks whether the page is in physical memory or on the disk.

Two virtual addresses can point to the same physical address (e.g. when two programs are sharing the same data).

### Replacement and Writes

To reduce the page fault rate, prefer least-recently used replacement.

* reference bit in page table set to 1 on access to page

* periodically cleared to 0 by the OS

* a page with reference bit = 0 has not been used recently

A disk write takes millions of cycles. So:

* write a whole block at once, rather than individual locations

* write through is impractical

* use write-back when replacing a page

* dirty bit in page table set when a page is written

    * a modified page is often called a dirty page

### Fast Translation Using a TLB (Translation Look-aside Buffer)

Address translation would appear to require extra memory references.

* one to access the page table entry

* then the actual memory access

But access to page tables has good locality.

* so use a fast cache of page table entries within the CPU

* called TLB

* typically 16-512 PTEs, 0.5-1 cycle for a hit, 10-100 cycles for miss, 0.01%-1% miss rate

* misses could be handled by hardware or software

* have to copy the valid, dirty, and reference bits from the page table

### TLB  Misses

Occurs when no entry in TLB matches a virtual address.

If the page is in memory, load the PTE from memory and try.

* could be handled in hardware

    * this gets complex for more complicated page table structures

* or in software

    * raise a special exception, with optimised handler

If the page is not in memory (page fault):

* OS handles fetching the page and updating the page table

* then restart the faulting instruction

### Page Fault Handler

1. Use faulting virtual address to find PTE.

2. Locate page on disk.

3. Choose page to replace.

    * if dirty, write to disk first

4. Read page into memory and update page table.

5. Make process runnable again.

    * restart from faulting instruction

### Example: TLB and Cache Interaction in FastMATH

If the cache tag uses physical address:

* need to translate (from virtual to physical) before cache lookup

If the cache tag uses virtual addresses:

* complications due to aliasing

    * different virtual addresses for shared physical address

### Memory Protection

Different tasks can share parts of their virtual address spaces.

* but need to protect against errant access

* requires OS assistance

Hardware support for OS protection:

* privileged supervisor mode (aka kernel mode)

* privileged instructions

* page tables and other state information only accessible in supervisor mode

* system call exception (e.g. syscall in MIPS)

# Common Framework for Memory Hierarchy

Common principles apply at all levels of the memory hierarchy.

* based on notions of caching

At each level in the hierarchy, the following are relevant:

* block placement

* finding a block

* replacement on a miss

* write policy

## Block Placement

Determined by associativity.

* direct-mapped (1-way associative)

    * only one choice for placement

* n-way set associative

    * n choices within a set

* fully associative

    * any location

Higher associativity reduces miss rate.

* increases complexity, cost, and access time

## Finding a Block

Hardware caches:

* reduce the number of comparisons to reduce the cost.

Virtual memory:

* full lookup table make full associativity feasible

* benefit in reduced miss rate

## Replacement

Choice of entry to replace on a miss:

* LRU

    * complex and costly hardware for high associativity

* random

    * close to LRU, easier to implement

[…]

## Write Policy

Write-through

* update both upper and lower levels

* simplifies replacement, but may require write buffer

Write-back

* update upper level only

* update lower level when block is replaced

* need to keep more state

Virtual memory

* only write-back is feasible, given disk write latency

## Sources of Misses

Compulsory misses (aka cold start misses)

* first access to a block that has never been in cache

Capacity misses

* due to finite cache size

* a replaced block is later accessed again

Conflict misses (aka collision misses)

* in a non-fully-associative cache

* due to competition for entries in a set

* would not occur in a fully associative cache of the same total size

## Cache Design Tradeoffs

* increase cache size

    * decreases capacity misses

    * may increase access time

* increase associativity

    * decrease conflict misses

    * may increase access time

* increase block size

    * decreases compulsory misses

    * increases miss penalty (have to swap out the whole block)

    * for very large block size, may increase miss rate due to pollution

# Finite State Machines

Note: important to know how the tag and index are converted to the address for checking if you have a cache hit – important for exams.

## Cache Controller FSM

* some optimisations are possible

# Cache Coherence Problem

* suppose two CPU cores share a physical address space

    * write-through caches

The memory view of the processors is based on their individual caches.

Each core has its own cache, but since both get their data from memory, how do we keep data in both coherent?

One core might write a value, and write-through will update that data in memory. This value might now be incorrect in the other core's cache.

# Coherence Defined

* reads return the most recently written value

* if P writes X and then reads X, the read should return the written value

* if P1 writes X and then P2 reads X, the read should also return the written value

* if P1 writes X and then P2 writes X, all processors should see the writes in the same order

    * then the final value of X will be consistent

## Cache Coherence Protocols

* cache in multiprocessors supports

    * migration of data to local caches (reduces bandwidth for shared memory)

    * replication of read-shared data (reduces contention for access)

        * contention in main memory is part of why it's slow – cache prevents this

Operations performed by caches in multiprocessors to ensure coherence

    * known as cache coherence protocol

    * the key part is state tracking of shared data

### Snooping Protocols

* Each cache monitors bus reads/writes

* sharing status of blocks copied to different caches

### Invalidating Snooping Protocols

* processor gets exclusive access to a block when it is to be written

    * broadcasts an invalidate message on the bus

    * subsequent read in another cache misses

        * owning cache supplies updated value

## Memory Consistency

When are writes seen by other processors?

* "seen" means a read returns the written value

    * can't be instantaneous

Assumptions:

* a write completes only when all processors have seen it

* a processor does not reorder writes with other accesses

    * so any processor that sees a write must have seen all previous writes

# Supporting Multiple Issue (ARM and Intel i7)

* Both have multi-banked caches that allow multiple accesses per cycle (assuming no back conflicts)

* core i7 optimisation

    * return requested word first

    * non-blocking cache for out-of-order processors

        * hit under miss: hides miss latency

        * miss under miss: overlaps miss latency

    * data pre-fetching

        * looks at pattern of data misses

        * predicts next address to start fetching data before a miss occurs

# Pitfall

* byte vs. word addressing

    * need to be clear on this

    * example: 32-byte direct-mapped cache, 4-byte blocks

        * byte 36 maps to block 1 (since 9 (= 36 / 4) modulo 8 (= 32 / 4) = 1)

        * word 36 maps to block 4 (since 36 modulo 8 = 4)

* ignoring memory system effects when writing or generating code

    * example: iterating over rows vs. columns of arrays

    * large strides result in poor locality

    * can drastically reduce performance

* in multiprocessor with shared L2 or L3 cache

    * less associativity than cores results in conflict misses

    * more cores -> need to increase associativity

* using AMAT (average memory access time) to evaluate performance of out-of-order processors

    * ignores effect of non-blocking accesses (being able to do other things while waiting for memory)

    * instead, evaluate performance by simulation

* implementing a VMM on an ISA not designed for virtualisation

    * e.g. non-privileged instructions accessing hardware resources

    * either extend the ISA or require the guest OS not to use problematic instructions

# Conclusion

* principle of locality is important

* fast memory is small, large memory is slow

    * caching gives the illusion of fast, large memory

* memory hierarchy

    * L1 <-> L2 <-> … <-> DRAM <-> disk

* memory system design is critical for multiprocessors

    * processors are very fast, but memory is slow – good design helps get good performance out of the CPU
